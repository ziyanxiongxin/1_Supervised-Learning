{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_validate\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score,roc_auc_score,accuracy_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import io\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "# uploaded = files.upload()\n",
    "# #extract the data\n",
    "# raw_df = pd.read_csv(io.BytesIO(uploaded['Skin_NonSkin.txt'], delimiter='\t'))\n",
    "\n",
    "\n",
    "# #extract the data\n",
    "raw_df = pd.read_csv('~/ML_class_projects/a_1/Skin_NonSkin.txt', delimiter='\t')\n",
    "cleaned_df = raw_df.copy()\n",
    "\n",
    "\n",
    "cleaned_df['class']=np.array([1 if x==1 else 0 for x in cleaned_df['class']])\n",
    "print(cleaned_df.head(5))\n",
    "#summarize data distribution\n",
    "y=np.array(cleaned_df['class'])\n",
    "X=np.array(cleaned_df.iloc[:,:-1])\n",
    "print(Counter(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=10\n",
    "cv = KFold(n_splits=t,shuffle=True)\n",
    "AUC_train_score_list=[]\n",
    "AUC_test_score_list=[]\n",
    "n_estimators_range=range(50,101,10)\n",
    "for n_estimator in n_estimators_range:\n",
    "    #define pipline\n",
    "    clf = tree.DecisionTreeClassifier(min_impurity_decrease=0.1)\n",
    "    abc =AdaBoostClassifier(n_estimators=n_estimator, base_estimator=clf,learning_rate=1)\n",
    "    scores =  cross_validate(abc, X, y, scoring=['roc_auc'], cv=cv, n_jobs=-1,return_train_score=True)\n",
    "#     recall_train_score_list.append(sum(scores['train_recall'])/t)\n",
    "#     recall_test_score_list.append(sum(scores['test_recall'])/t)\n",
    "    \n",
    "    AUC_train_score_list.append(sum(scores['train_roc_auc'])/t)\n",
    "    AUC_test_score_list.append(sum(scores['test_roc_auc'])/t)\n",
    "#     fit_time_list.append(sum(scores['fit_time'])/t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#generate Fig_1.3.0 tunning curves\n",
    "fig_0, axs = plt.subplots(1, 1, figsize=(10, 5), sharey=True)\n",
    "axs.plot(n_estimators_range,AUC_train_score_list,\"r^\",linestyle = \"--\",label='training data')\n",
    "axs.plot(n_estimators_range,AUC_test_score_list,\"b^\",linestyle = \"--\",label='test data')\n",
    "axs.set_xlabel(\"The maximum number of estimators: n_estimators\")\n",
    "axs.set_ylabel(\"AUC score\")\n",
    "axs.legend()\n",
    "fig_0.suptitle(\"Fig_2.3.0: AUC score(Ada tunning)\")\n",
    "fig_0.savefig(\"Ada_fig_2.3_tunning:AP score.png\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy_train_score=[]\n",
    "Accuracy_test_score=[]\n",
    "Precision_train_score=[]\n",
    "Precision_test_score=[]\n",
    "training_time = []\n",
    "\n",
    "for k in range(4,11):\n",
    "    cv = KFold(n_splits=k,shuffle=True)\n",
    "    clf = tree.DecisionTreeClassifier(min_impurity_decrease=0.1)\n",
    "    abc =AdaBoostClassifier(n_estimators=80, base_estimator=clf,learning_rate=1)\n",
    "    scores =  cross_validate(abc, X, y, scoring=['accuracy','precision'], cv=cv, n_jobs=-1,return_train_score=True)\n",
    "    Accuracy_train_score.append(sum(scores['train_accuracy'])/k)\n",
    "    Accuracy_test_score.append(sum(scores['test_accuracy'])/k)\n",
    "    Precision_train_score.append(sum(scores['train_precision'])/k)\n",
    "    Precision_test_score.append(sum(scores['test_precision'])/k)\n",
    "    training_time.append(sum(scores['fit_time'])/k)\n",
    "    \n",
    "\n",
    "K= range(4,11)\n",
    "fig_1, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=False)\n",
    "axs[0].plot(K,training_time,\"r^\",linestyle = \"--\",label=\"traing_time:seconds\")\n",
    "axs[0].set_xlabel(\"training size k\")\n",
    "axs[0].set_ylabel(\"training time\")\n",
    "axs[1].plot(K,Accuracy_train_score,\"r^\",linestyle = \"--\",label='training data')\n",
    "axs[1].plot(K,Accuracy_test_score,\"b^\",linestyle = \"--\",label='test data')\n",
    "axs[1].set_xlabel(\"training size k\")\n",
    "axs[1].set_ylabel(\"accuracy_score\")\n",
    "axs[2].plot(K,Precision_train_score,\"r^\",linestyle = \"--\",label='training data')\n",
    "axs[2].plot(K,Precision_test_score,\"b^\",linestyle = \"--\",label='test data')\n",
    "axs[2].set_xlabel(\"training size k\")\n",
    "axs[2].set_ylabel(\"precision_score\")\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "axs[2].legend()\n",
    "fig_1.suptitle(\"Fig_2.3.1: learning curves(Ada boosting)\")\n",
    "\n",
    "fig_1.savefig(\"Ada_fig_2.3_traninig:learning curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############This cell is to compare decision tree without and with boosting: generate the data in table_2.3.0 in the write up##\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/8, random_state=42)\n",
    "clf = tree.DecisionTreeClassifier(min_impurity_decrease=0.1)\n",
    "clf.fit(X_train,y_train)\n",
    "print(\"base decision estimator's tree depth is:\",clf.get_depth())\n",
    "print(\"accuracy and precision scores for training data before boosting are:\",accuracy_score(y_train,clf.predict(X_train)),precision_score(y_train,clf.predict(X_train)))\n",
    "print(\"accuracy and precision for test data before boosting are:\",accuracy_score(y_test,clf.predict(X_test)),precision_score(y_test,clf.predict(X_test)))\n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(min_impurity_decrease=0.1)\n",
    "abc =AdaBoostClassifier(n_estimators=80, base_estimator=clf,learning_rate=1)\n",
    "abc.fit(X_train,y_train)\n",
    "\n",
    "print(\"accuracy and precision scores for training data after boosting are:\",accuracy_score(y_train,abc.predict(X_train)),precision_score(y_train,abc.predict(X_train)))\n",
    "print(\"accuracy and precision for test data after boosting are:\",accuracy_score(y_test,abc.predict(X_test)),precision_score(y_test,abc.predict(X_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
